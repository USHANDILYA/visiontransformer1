{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 224ms/step - loss: 1.1209 - accuracy: 0.7083 - val_loss: 0.3840 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.1845 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.2816e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 1.9602e-04 - accuracy: 1.0000 - val_loss: 8.7972e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 5.8256e-05 - accuracy: 1.0000 - val_loss: 3.2802e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 2.6195e-05 - accuracy: 1.0000 - val_loss: 1.5437e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 1.1931e-05 - accuracy: 1.0000 - val_loss: 8.7022e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 7.2220e-06 - accuracy: 1.0000 - val_loss: 5.6028e-06 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 4.8677e-06 - accuracy: 1.0000 - val_loss: 4.0134e-06 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.0134e-06 - accuracy: 1.0000\n",
      "Test Loss: 4.013370471511735e-06, Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_har_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vit_har_model\\assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "NUM_FRAMES = 16\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "NUM_CLASSES = 5  # Update based on your dataset\n",
    "\n",
    "# Load ViT model and feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Function to load and preprocess video frames\n",
    "def load_video_frames(video_path, num_frames=NUM_FRAMES, img_size=IMG_SIZE):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i in frame_indices:\n",
    "            frame = cv2.resize(frame, (img_size, img_size))\n",
    "            frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "    if len(frames) < num_frames:\n",
    "        padding = num_frames - len(frames)\n",
    "        frames = np.pad(frames, ((0, padding), (0, 0), (0, 0), (0, 0)), mode='constant')\n",
    "    return frames\n",
    "\n",
    "# Function to extract features using ViT\n",
    "def extract_vit_features(frames):\n",
    "    inputs = feature_extractor(images=list(frames), return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(**inputs).last_hidden_state\n",
    "    return outputs[:, 0, :].numpy()  # [CLS] token features\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(dataset_path):\n",
    "    X, y = [], []\n",
    "    action_classes = sorted(os.listdir(dataset_path))\n",
    "    class_to_idx = {action: idx for idx, action in enumerate(action_classes)}\n",
    "\n",
    "    for action in action_classes:\n",
    "        action_folder = os.path.join(dataset_path, action)\n",
    "        for video in os.listdir(action_folder):\n",
    "            video_path = os.path.join(action_folder, video)\n",
    "            frames = load_video_frames(video_path)\n",
    "            features = extract_vit_features(frames)\n",
    "            X.append(features)\n",
    "            y.append(class_to_idx[action])\n",
    "\n",
    "    return np.array(X), np.array(y), class_to_idx\n",
    "\n",
    "# Load data\n",
    "DATASET_PATH = r\"C:\\Users\\UTKARSH\\Desktop\\data science\\dl\\cvt\\data\"  # Update with your dataset path\n",
    "X, y, class_to_idx = load_dataset(DATASET_PATH)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    input_layer = tf.keras.Input(shape=(NUM_FRAMES, 768))  # 768 is the ViT hidden size\n",
    "    lstm_layer = tf.keras.layers.LSTM(128)(input_layer)\n",
    "    dense_layer = tf.keras.layers.Dense(128, activation=\"relu\")(lstm_layer)\n",
    "    output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(dense_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"vit_har_model\")  # Saves in SavedModel format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0134e-06 - accuracy: 1.0000\n",
      "Test Loss: 4.013370471511735e-06\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
